# config.toml for OSCAR-C

# This file controls the behavior and parameters of the OSCAR-C agent.
# You can adjust these values to experiment with different agent personalities,
# learning rates, and operational characteristics.

# --- Filesystem Interaction Controls ---
# Defines how the agent can interact with your computer's files.
[filesystem]
max_list_items = 25              # Max number of files/folders to list in one go from a directory.
max_read_chars = 3500            # Max characters to read from a single file at once.
allow_file_write = true         # Can the agent create or modify files? (true/false) SECURITY RISK IF TRUE.
allow_overwrite = false          # If writing is allowed, can it overwrite existing files? (true/false)

# --- Core Agent Settings ---
# General operational parameters for the agent.
[agent]
pid_file_name = "oscar_c.pid"    # Name of the file that stores the agent's process ID when running.
ui_meter_update_interval_s = 2.0 # How often (in seconds) to send status updates to the User Interface.
goal_re_evaluation_interval_cycles = 25 # After how many thinking cycles should the agent reconsider its current goals.
max_consecutive_planning_failures = 3 # How many times can planning for a goal fail consecutively before the goal itself is marked as failed.
max_execution_failures_per_goal = 3 # How many times can actions for a specific goal fail before that goal is marked as failed.

# Settings for the agent's default "Observe and learn" goal.
default_goal_cooldown_cycles = 15 # Min thinking cycles before another "Observe and learn" goal can be generated.
min_curiosity_for_observe = 0.75  # Min curiosity level (0.0-1.0) needed to generate an "Observe and learn" goal.

# --- Data Storage Paths ---
# Specifies where the agent saves its persistent data relative to its main directory.
[agent_data_paths]
pid_directory = "run"                     # Folder for runtime files like the PID file.
kb_db_path = "data/oscar_c_kb.db"         # Path to the Knowledge Base (memory) SQLite database file.
narrative_log_path = "data/narrative_log.jsonl" # Path to the agent's autobiographical narrative log file.
self_model_path = "data/self_model.json"  # Path to where the agent saves its understanding of itself.
predictive_model_path = "data/predictive_model_OLD.json" # Path for the agent's model of how the world responds to actions.
predictive_model_learned_data_path = "data/pwm_learning_buffer.csv" # <<< NEW: For CBN data
# or "data/pwm_fitted_model.bif" if saving fitted model
performance_adjustments_path = "data/perf_adjustments.json" # Path for saved performance tuning settings.
# --- Add path for HTN library (Phase III - C.3)
htn_library_path = "data/htn_library.json" # For learned HTN methods

# --- Global Workspace (Short-Term "Conscious" Buffer) ---
# Settings for the agent's GWT-inspired global workspace.
[global_workspace]
capacity = 7                          # How many items can be "in focus" or "conscious" at once. (Inspired by Miller's 7 +/- 2).
broadcast_threshold = 0.1             # Minimum attention score (0.0-1.0) an item needs to enter the workspace.
min_items_if_any_attended = 1       # If anything has attention, ensure at least this many items make it to the workspace (even if slightly below threshold).

# --- Attention Controller ---
# How the agent decides what to pay attention to.
[attention_controller]
recency_weight = 0.3                  # How much importance to give to newer information (0.0-1.0).
hint_weight = 0.5                     # How much to value explicit "hints" from other components about an item's importance (0.0-1.0).
goal_relevance_weight = 0.2           # How much importance to give to information related to the current goal (0.0-1.0).
max_candidates = 8                    # Max number of items considered for attention in one cycle. (Adjusted by PerformanceOptimizer).
softmax_temperature = 1.0             # Controls sharpness of attention allocation (lower = sharper focus on top items).
novelty_window_size = 3               # How many past workspace contents to remember to check for newness.
novelty_bonus_weight = 0.2            # Extra attention for new, unseen items (0.0-1.0).
surprise_bonus_weight = 0.4           # Extra attention for surprising events (when predictions fail) (0.0-1.0).
pain_attention_distraction_factor = 0.8 # How much "pain" intensity (0-10) makes painful memories grab attention (0.0-1.0).
pain_inactive_reset_cycles = 20 # Default: after 20 cycles of not being in GWM
pain_rumination_window_multiplier = 3 # Default: history is 3x the suppression threshold length
pain_rumination_threshold_cycles = 2

# Factor by which a pain event's attention score (specifically its hint_score component)
# is multiplied if it's suppressed due to rumination. (0.0 to 1.0, lower means stronger suppression)
pain_rumination_suppression_factor = 0.1 # Default: reduce score to 10%

# --- Consciousness Level Assessor ---
# How the agent estimates its own level of "consciousness" using a proxy metric (Phi-Proxy).
[consciousness_assessor]
meta_conscious_threshold = 0.75       # Score (0-1) needed for "Meta-Conscious" state (highly aware, self-referential).
conscious_threshold = 0.50            # Score (0-1) needed for "Conscious" state.
pre_conscious_threshold = 0.25        # Score (0-1) needed for "Pre-Conscious" state (dimly aware).
unconscious_threshold = 0.10          # Score (0-1) below which agent is "Unconscious" (but still above deep unconscious).
diff_weight_sources = 0.4             # Importance of having many distinct information sources.
diff_weight_lexical = 0.6             # Importance of diverse vocabulary in workspace content.
int_weight_shared_concepts = 0.5      # Importance of shared concepts/keywords across workspace items.
phi_contrib_diff = 0.5                # How much differentiation contributes (0.0-1.0).
phi_contrib_int = 0.5                 # How much integration contributes (0.0-1.0).
global_workspace_capacity_for_norm = 5 # Used to normalize "distinct_source_count" (should match global_workspace.capacity).
# --- Add config for PhiCalculator integration (Phase III - C.2)
phi_calculator_integration_weight = 0.3 # Weight for phi_value_from_calc in final score (0.0-1.0)
cla_stopwords = ["a", "the", "is"] # Example
cla_goal_keywords = ["goal", "objective", "task"] # Example
cla_coherence_edge_threshold = 0.05 # Threshold for relationship strength to form an edge in coherence graph (0.0-1.0)
differentiation_norm_factor = 20.0 # <<< ADD THIS (For normalizing item count in phenomenal state for DI_Calc)

# --- Performance Tuning ---
# Settings related to agent's operational speed and self-optimization.
[performance]
target_cycle_time = 2.0               # Ideal duration (in seconds) for one full thinking cycle.
max_planning_depth = 3                # Max depth for HTN planning search. (Adjusted by PerformanceOptimizer).
profiler_history_size = 100           # How many past cycle performance profiles to keep for analysis.

# --- Loop Detection ---
# How the agent detects if it's stuck in repetitive action loops.
[loop_detection]
window_size = 3                       # Number of recent actions to check for loops. (Adjusted by PerformanceOptimizer).
max_consecutive_actions = 2           # Max times the exact same action can repeat before flagging.
frequency_threshold = 0.67            # If one action type makes up this % of actions in window, flag it (0.0-1.0).
ignore_thinking_actions = true        # Should "THINKING" actions be ignored when checking for loops? (true/false).

# --- Knowledge Base (Long-Term Memory) ---
[knowledge_base]
default_context_retrieval_count = 3   # How many related facts to retrieve from memory by default for context.

# --- Cognitive Cache (Short-Term Memory for Computations) ---
[cognitive_cache]
default_ttl = 0.2                     # Default time-to-live (seconds) for cached computation results.

# --- Telemetry (For Developers/Dashboard) ---
# Settings for sending agent status data to an external dashboard.
[telemetry]
enabled = false                       # Enable/disable telemetry (true/false).
host = "localhost"                    # Dashboard server host.
port = 8765                           # Dashboard server port.

# --- Error Recovery System ---
# How the agent handles internal errors.
[error_recovery]
max_error_history = 5                 # How many recent errors to remember.
frequency_window = 3                  # Check error frequency over this many recent errors.
frequency_threshold = 2               # If same error type occurs this many times in window, escalate recovery.

# --- Dynamic Self-Model (Agent's Understanding of Itself) ---
[dynamic_self_model]
learning_rate = 0.1                   # General learning rate for updating self-model confidences (0.0-1.0).
max_invalid_paths = 50                # Max number of known bad file paths to remember.
reflection_interval_cycles = 5      # How often (agent cycles) to perform deep self-reflection.
learning_events_history_size = 10     # How many learning events (action outcomes) to store for reflection.

# --- Emergent Motivation System (Internal Drives) ---
[emergent_motivation_system]
detailed_evaluation_interval_cycles = 3 # How often to re-evaluate all drives.
ems_cs_history_maxlen = 5               # How many recent ConsciousnessState levels to remember for drive calculations.
ems_low_cs_persistence_threshold = 3    # If this many recent CS levels were LOW, trigger specific drive effects.
ems_low_cs_curiosity_boost_factor = 0.2 # <<< INCREASED SIGNIFICANTLY FOR TESTING (was 0.1)

[emergent_motivation_system.drives.curiosity]
gain_prediction_error = 0.05
gain_from_high_pain_for_distraction = 0.03
gain_from_low_purpose_for_exploration = 0.1
threshold_high_pain_for_curiosity = 7.0
threshold_low_purpose_for_curiosity = 4.0
gain_entropy_uncertainty = 0.4 # <<< ADD THIS (How much model's outcome entropy boosts curiosity)
# decay = 0.02 (using default from code if not here)
# gain_discovery = 0.08 (using default from code if not here)

[emergent_motivation_system.drives.satisfaction]
loss_from_pain_factor = 0.1
gain_from_happiness_factor = 0.05

[emergent_motivation_system.drives.competence]
gain_from_low_purpose_for_efficacy = 0.08
threshold_low_purpose_for_competence = 4.5

# --- State History Logger ---
# For tracking component states and cycle snapshots for analysis (e.g., causal density, advanced phi).
[state_history_logger]
max_history_per_component = 50   # Max number of status updates to keep per component.
max_cycle_snapshots = 100        # Max number of full cognitive cycle snapshots to keep.

# --- Narrative Constructor (Agent's Autobiography) ---
[narrative_constructor]
max_length = 100
valence_change_threshold = 0.2
intensity_threshold = 0.6
save_interval_s = 10
drive_change_threshold = 0.1
# significance_threshold = 0.1 # This seemed to be replaced by more specific thresholds below
pain_change_threshold_sig = 0.5
happiness_change_threshold_sig = 0.75
purpose_change_threshold_sig = 0.5
# timeout_s = 10.0 # This specific timeout will now be handled by oscar_specialized_llms section

# --- Predictive World Model (Agent's Understanding of Cause & Effect) ---
[predictive_world_model]
initial_version = 0
learning_rate = 0.1
memory_length = 5
save_interval_versions = 10
# --- Add config for PWM CBN (Phase III - C.4)
cpd_reestimation_trigger_count_frequent = 2 # How many new observations before re-estimating CPDs (frequent updates)
cbn_config_file = "data/pwm_cbn_config.json" # <<< NEW: Path to CBN structure config
cbn_query_cache_ttl_s = 0.2 # Cache CBN query results for 0.2 seconds
cbn_target_outcome_node = "OutcomeNode" # Explicitly define target prediction node (matches default)

# --- Performance Optimizer ---
[performance_optimizer]
history_size = 5
auto_apply_adjustments = true

# --- Experience Stream (Integrates info into PhenomenalState) ---
[experience_stream]
intensity_factor = 1.0
valence_decay = 0.1
custom_stopwords = ["oscar", "agent"]

# --- Meta-Cognitive Monitor (Agent "Thinking About Thinking") ---
[meta_cognition]
stagnation_threshold_s = 0.2 # Renamed from stagnation_threshold for clarity
failure_rate_threshold = 0.6
low_consciousness_threshold_s = 0.2 # Renamed from low_consciousness_threshold for clarity
history_size = 3
reflection_trigger_frequency_cycles = 3

# --- LLM (Large Language Model) Settings ---
[llm_settings]
# Default Ollama model to use if a specialized one isn't specified for a task
# Replace "qwen2:4b-instruct" with your actual Qwen3 4B Ollama tag
default_llm_model = "qwen3:4b"
default_timeout_s = 180.0
# New global default for the 'think:true' API flag.
# Set to true if you want most LLM calls to attempt to get a thinking trace.
# Can be overridden by specific specialized_llms settings.
default_enable_thinking = true
action_selection_temperature = 0.7 # General temperature for LLM-assisted action selection (if used)
max_recent_turns_for_prompt = 3    # For conversational contexts, how many past turns to include
intent_mapping_temperature = 0.3   # Default temperature for mapping user text to goals/tasks (if LLM used)

# --- Specialized LLMs and LoRAs ---
# Configure specific Ollama models (base + LoRA or fine-tuned models) for particular tasks.
[oscar_specialized_llms]
# Persona LoRA Model (as per qwen3 lora guide.txt Phase 5.1)
persona_dialogue_model = "oscarcv4"     # Matches 'ollama create' name
persona_dialogue_enable_thinking = true       # Enable 'think:true' for persona introspection
persona_dialogue_temperature = 0.65            # Specific temperature for persona responses
persona_dialogue_timeout_s = 120.0             # Specific timeout for persona responses

# Narrative LLM (as per Ollama thinking mode integration plan)

narrative_llm_model = "oscarcv4"        # Replace "qwen2:4b-instruct" if NarrativeConstructor uses a different base or LoRA
narrative_llm_enable_thinking = false           # Enable 'think:true' for richer narratives
narrative_llm_temperature = 0.75               # Specific temperature for narrative generation
narrative_llm_timeout_s = 150.0                # Specific timeout for narrative generation

# HTN Planner LLM Sketching (as per Ollama thinking mode integration plan)
# Replace "qwen2:4b-instruct" if HTN uses a different base or LoRA
htn_llm_sketch_model = "qwen3:4b"
htn_llm_sketch_enable_thinking = true          # Enable 'think:true' for debugging plan sketches
htn_llm_sketch_temperature = 0.2               # Lower temperature for more deterministic sketches
htn_llm_sketch_timeout_s = 90.0                # Specific timeout for plan sketching

# AgentController Intent Mapping (User Goal Mapping via LLM)
# Replace "qwen2:4b-instruct" if intent mapping uses a different base or LoRA
intent_mapping_model = "qwen3:4b"
intent_mapping_enable_thinking = true          # Enable 'think:true' for debugging intent parsing
intent_mapping_temperature = 0.3               # Lower temperature for more precise mapping
intent_mapping_timeout_s = 60.0                # Specific timeout for intent mapping

# Example from loras_for_oscar.txt (General Computer/Networking Skills LoRAs)
# These would be added as you develop and deploy them with Ollama.
# cli_generator_model = "oscar_lora_cli_generator"
# cli_generator_enable_thinking = true # Thinking might not be needed for direct command generation
# cli_generator_temperature = 0.2
# cli_generator_timeout_s = 30.0

# cli_interpreter_model = "oscar_lora_cli_interpreter"
# cli_interpreter_enable_thinking = true # Thinking could be useful for summarizing output
# cli_interpreter_temperature = 0.5
# cli_interpreter_timeout_s = 45.0

# concept_explainer_model = "oscar_lora_os_concept_explainer"
# concept_explainer_enable_thinking = true
# concept_explainer_temperature = 0.7
# concept_explainer_timeout_s = 60.0

# troubleshooting_model = "oscar_lora_os_troubleshooting_advisor"
# troubleshooting_enable_thinking = true # Thinking useful for step-by-step diagnostics
# troubleshooting_temperature = 0.4
# troubleshooting_timeout_s = 90.0


# --- HTN Planner (Hierarchical Task Network Planner) ---
[htn_planner]
plan_cache_ttl_s = 0.2
min_planning_depth_on_low_cs = 1
max_planning_depth_on_low_cs = 2
low_cs_simplicity_penalty_factor = 0.5
# --- Add config for HTN method learning (Phase III - C.3)
# llm_sketch_timeout_s and llm_sketch_temperature are now under [oscar_specialized_llms]

[htn_planner.htn_planner_pruning]
min_usage_for_pruning = 10      # Method must be used at least this many times before considering pruning due to low success/confidence.
low_success_rate_threshold = 0.2 # If success rate < 20% (and confidence also low) after min_usage, prune.
low_confidence_threshold = 0.3   # If confidence < 30% (and success also low) after min_usage, prune.
max_age_unused_days = 30         # Prune learned methods (with non-max confidence) if not used for this many days

# --- Internal State System (Pain, Happiness, Purpose) ---
[internal_states]
baseline_pain_age_factor = 0.00001
max_baseline_pain_from_age = 1.0
acute_pain_goal_fail_priority_scale_factor = 0.3
pain_from_planning_failure_scale_factor = 0.25 # MDP II.A.3
pain_from_execution_failure_scale_factor = 0.35 # MDP II.A.3
pain_event_max_initial_intensity = 2.0
default_pain_event_decay_rate_per_cycle = 0.005
pain_event_min_intensity_to_retain = 0.01
happiness_from_goal_priority_scale_factor = 0.3
happiness_decay_to_baseline_factor = 0.03
happiness_baseline_target = 5.0
pain_impact_on_happiness_scale_factor = 0.2
purpose_from_capability_gain_factor = 0.1
purpose_from_high_priority_goal_factor = 0.4
purpose_decay_rate_per_cycle = 0.0005
complex_goal_priority_threshold = 4.5
max_pain_shutdown_threshold = 9.0
min_purpose_shutdown_threshold = 1.0

# --- Value System ---
[value_system]
plan_rejection_value_threshold = -0.3
action_safety_veto_threshold = -0.7
safety_critical_paths_write = [
    "/bin", "/boot", "/dev", "/etc", "/lib", "/lib64", "/proc", "/root", "/run", "/sbin", "/sys", "/usr",
    "c:/windows", "c:/program files", "c:/program files (x86)"
]
safety_modification_trigger_threshold = -0.6 # From MDP C.1.4

[value_system.value_weights]
# safety = 2.5
# efficiency = 0.7

[value_system.tradeoff_matrix]
    [value_system.tradeoff_matrix.SAFETY]
    EFFICIENCY = 0.95
    KNOWLEDGE_GAIN = 0.9

    [value_system.tradeoff_matrix.TRUTHFULNESS]
    USER_SATISFACTION = 0.7

# --- PhiCalculator specific config (Phase III - C.2) ---
[phi_calculator] # Placeholder, not used yet by PhiCalculator class
max_partitions_to_evaluate = 100 # For PhiCalculator._calculate_minimum_information_bipartition

# --- Computational Budgets (Phase III General Management) ---
[computation_budgets] # Placeholder, components need to implement budget checking
phi_calculator_max_time_ms_per_cycle = 10
pwm_cbn_fit_max_time_ms_overall = 500
htn_method_learning_max_time_ms = 1000

# --- OS Integration (Phase IV - D.2) ---
[os_integration]
pain_threshold_cpu = 90.0
pain_threshold_memory = 85.0
# For restricted EXECUTE_COMMAND:
allowed_commands = [
    # Examples, paths need to be relative to sandbox or use a placeholder.
    # Actual parameterization is complex and handled by ActionExecutor.
    # "cat <filepath_in_sandbox>",
    # "echo <string_to_file_in_sandbox>",
    # "curl -s -o <output_file_in_sandbox> <safe_http_url>",
    # "ls <path_in_sandbox>"
]
sandbox_dir = "agent_os_sandbox" # Relative to agent_root_path
allowed_http_domains_for_curl = ["example.com", "api.open-meteo.com"] # For curl example